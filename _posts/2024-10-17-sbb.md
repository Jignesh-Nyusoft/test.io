---
title: SWT-Bench
short: A Benchmark for Testing and Validating Bugfixes
date: 2024-10-17
layout: newsdetail
image: "/assets/images/website/postimges/post-img-1.png"
---

<!-- ![image]({{ "/assets/images/webisite/postimges/post-img-1.jpeg" | relative_url }}) -->


<!-- On the morning of August 20, 2024, SBB (Swiss Federal Railways) faced a significant [disruption](https://news.sbb.ch/artikel/130813/nach-netzwerk-stoerung-situation-weitgehend-normalisiert) caused by a network issue. This incident triggered widespread service interruptions, affecting ticket sales, passenger information systems, and regional train operations. The fallout from this outage led to delays of up to 60 minutes, with residual disruptions persisting throughout the day. Although safety systems remained unaffected, this event highlights the fragility of our critical infrastructure.

This event underscores a harsh reality: critical infrastructure is more fragile than we often realize. When the systems we rely on for essential services break down, the consequences can ripple across entire regions, causing chaos and frustration.

Addressing these issues swiftly and effectively is no small feat, especially without the right tools to provide comprehensive insights and control. In the realm of network management, the complexity of modern systems means that pinpointing the root cause of a problem can be akin to searching for a needle in a haystack—a challenge that only intensifies as networks expand.

At NetFabric, we are committed to transforming this landscape. Our mission is building a next-generation network observability platform that unifies all your network data sources into a cohesive, seamless experience, delivering real-time answers to every network question. The future of your network doesn’t have to be uncertain. With NetFabric, you can be prepared for whatever comes next. -->

We are excited to share <a href="https://github.com/logic-star-ai/SWT-Bench">SWT-Bench</a>, the first benchmark for reproducing bugs and validating their fixes based on (GitHub) issue descriptions. We presented SWT-Bench at two ICML workshops and want to thank everyone who stopped by for their interest, enthusiasm, and the great discussions we had. We now see a community trend to not only focus on fixing bugs but also generating tests that can effectively reproduce them and validate that proposed fixes truly resolve the issues. We believe this is essential for achieving truly autonomous bug fixing, which is what LogicStar delivers.

In our paper, we demonstrate how any code repair benchmark with a known ground truth solution can be transformed into a test generation and issue reproduction benchmark. There, the goal is to create a “reproducing test” that fails on the original codebase and passes after the ground truth fix has been applied. Our analysis shows that Code Agents excel in this task and outperform dedicated LLM-based test generation methods. Leveraging these tests for code repair further allows us to significantly enhance precision. To learn more, please check out our preprint <a href="https://arxiv.org/abs/2406.12952">paper</a>).

<a href="https://logicstar.ai/">LogicStar.AI</a>  builds on top of this research to achieve a truly autonomous bug fixing that you can trust as you trust your top engineers. 

